{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing: From Skeletal Coordinates to Spatio-Temporal Graphs\n",
        "\n",
        "**Project:** Unsupervised Motor Signatures in ASD\n",
        "\n",
        "**Paper Reference:** *Unsupervised Deep Learning Framework for Quantifying Atypical Motor Signatures in ASD*\n",
        "\n",
        "## Overview\n",
        "This notebook outlines the pipeline for transforming raw 3D skeletal data (extracted via MediaPipe) into the specific graph tensors required by the STGCN-AE model.\n",
        "\n",
        "**Key Steps:**\n",
        "1.  **Graph Construction:** Converting frame-by-frame joint coordinates into a fully connected graph structure ($N=24$ nodes).\n",
        "2.  **Temporal Trimming:** Retaining only the final 120 frames (approx. 4 seconds) of the action to isolate the core motor behavior.\n",
        "3.  **Sliding Window Segmentation:** slicing the sequence into 2-second overlapping windows (60 frames) with a 0.5-second stride."
      ],
      "metadata": {
        "id": "SwIf92BNak4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "# 1. Force install PyTorch 2.8.0 with CUDA 12.6\n",
        "# We use --upgrade --force-reinstall to overwrite whatever Colab loaded by default\n",
        "!pip install torch==2.8.0+cu126 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126 --upgrade\n",
        "\n",
        "# 2. Verify the version before proceeding\n",
        "import torch\n",
        "print(f\"Successfully installed PyTorch version: {torch.__version__}\")\n",
        "\n",
        "# 3. Set environment variable for PyG (PyTorch Geometric) installation\n",
        "# This tells pip explicitly which binary wheels to grab\n",
        "os.environ['TORCH'] = \"2.8.0+cu126\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpKtpFH2ghr4",
        "outputId": "21da74eb-084b-42c3-d779-d40b31117e55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu126\n",
            "Requirement already satisfied: torch==2.8.0+cu126 in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Collecting torchvision\n",
            "  Using cached https://download.pytorch.org/whl/cu126/torchvision-0.24.1%2Bcu126-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Collecting torchaudio\n",
            "  Using cached https://download.pytorch.org/whl/cu126/torchaudio-2.9.1%2Bcu126-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0+cu126) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0+cu126) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0+cu126) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0+cu126) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0+cu126) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0+cu126) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0+cu126) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0+cu126) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0+cu126) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0+cu126) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0+cu126) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0+cu126) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0+cu126) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0+cu126) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0+cu126) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0+cu126) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0+cu126) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0+cu126) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0+cu126) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0+cu126) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0+cu126) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0+cu126) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchvision\n",
            "  Using cached https://download.pytorch.org/whl/cu126/torchvision-0.24.0%2Bcu126-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "INFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchaudio\n",
            "  Using cached https://download.pytorch.org/whl/cu126/torchaudio-2.9.0%2Bcu126-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.8.0+cu126) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.8.0+cu126) (3.0.3)\n",
            "Successfully installed PyTorch version: 2.8.0+cu126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4qfGc9JSkCH",
        "outputId": "7c388ad3-930c-4b0a-8085-8e4e838e25e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch_geometric in /usr/local/lib/python3.12/dist-packages (2.7.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.13.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.22.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch_geometric) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2025.11.12)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->torch_geometric) (4.15.0)\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.8.0+cu126.html\n",
            "Collecting pyg_lib\n",
            "  Downloading https://data.pyg.org/whl/torch-2.8.0%2Bcu126/pyg_lib-0.5.0%2Bpt28cu126-cp312-cp312-linux_x86_64.whl (4.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.8.0%2Bcu126/torch_scatter-2.1.2%2Bpt28cu126-cp312-cp312-linux_x86_64.whl (10.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.8.0%2Bcu126/torch_sparse-0.6.18%2Bpt28cu126-cp312-cp312-linux_x86_64.whl (5.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-2.8.0%2Bcu126/torch_cluster-1.6.3%2Bpt28cu126-cp312-cp312-linux_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_spline_conv\n",
            "  Downloading https://data.pyg.org/whl/torch-2.8.0%2Bcu126/torch_spline_conv-1.2.2%2Bpt28cu126-cp312-cp312-linux_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from torch_sparse) (1.16.3)\n",
            "Requirement already satisfied: numpy<2.6,>=1.25.2 in /usr/local/lib/python3.12/dist-packages (from scipy->torch_sparse) (2.0.2)\n",
            "Installing collected packages: torch_spline_conv, torch_scatter, pyg_lib, torch_sparse, torch_cluster\n",
            "Successfully installed pyg_lib-0.5.0+pt28cu126 torch_cluster-1.6.3+pt28cu126 torch_scatter-2.1.2+pt28cu126 torch_sparse-0.6.18+pt28cu126 torch_spline_conv-1.2.2+pt28cu126\n",
            "Collecting torch-geometric-temporal\n",
            "  Using cached torch_geometric_temporal-0.56.2-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: decorator==4.4.2 in /usr/local/lib/python3.12/dist-packages (from torch-geometric-temporal) (4.4.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from torch-geometric-temporal) (2.8.0+cu126)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.12/dist-packages (from torch-geometric-temporal) (3.0.12)\n",
            "Requirement already satisfied: torch-sparse in /usr/local/lib/python3.12/dist-packages (from torch-geometric-temporal) (0.6.18+pt28cu126)\n",
            "Requirement already satisfied: torch-scatter in /usr/local/lib/python3.12/dist-packages (from torch-geometric-temporal) (2.1.2+pt28cu126)\n",
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.12/dist-packages (from torch-geometric-temporal) (2.7.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch-geometric-temporal) (2.0.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch-geometric-temporal) (3.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->torch-geometric-temporal) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->torch-geometric-temporal) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->torch-geometric-temporal) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->torch-geometric-temporal) (1.14.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->torch-geometric-temporal) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->torch-geometric-temporal) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->torch-geometric-temporal) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->torch-geometric-temporal) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->torch-geometric-temporal) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->torch-geometric-temporal) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->torch-geometric-temporal) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->torch-geometric-temporal) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->torch-geometric-temporal) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->torch-geometric-temporal) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->torch-geometric-temporal) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->torch-geometric-temporal) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->torch-geometric-temporal) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->torch-geometric-temporal) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->torch-geometric-temporal) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->torch-geometric-temporal) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->torch-geometric-temporal) (3.4.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch-geometric->torch-geometric-temporal) (3.13.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch-geometric->torch-geometric-temporal) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch-geometric->torch-geometric-temporal) (3.2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch-geometric->torch-geometric-temporal) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch-geometric->torch-geometric-temporal) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from torch-geometric->torch-geometric-temporal) (3.6.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from torch-sparse->torch-geometric-temporal) (1.16.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->torch-geometric-temporal) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric->torch-geometric-temporal) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric->torch-geometric-temporal) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric->torch-geometric-temporal) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric->torch-geometric-temporal) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric->torch-geometric-temporal) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric->torch-geometric-temporal) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric->torch-geometric-temporal) (1.22.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->torch-geometric-temporal) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric->torch-geometric-temporal) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric->torch-geometric-temporal) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric->torch-geometric-temporal) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric->torch-geometric-temporal) (2025.11.12)\n",
            "Using cached torch_geometric_temporal-0.56.2-py3-none-any.whl (102 kB)\n",
            "Installing collected packages: torch-geometric-temporal\n",
            "Successfully installed torch-geometric-temporal-0.56.2\n"
          ]
        }
      ],
      "source": [
        "!pip install torch_geometric\n",
        "\n",
        "# Optional dependencies:\n",
        "!pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install torch-geometric-temporal"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "def generate_dummy_dataset(output_dir='sample_data', num_subjects=4, num_frames=150):\n",
        "    \"\"\"\n",
        "    Generates a synthetic dataset to mimic the structure required by the ASD Motor pipeline.\n",
        "\n",
        "    Args:\n",
        "        output_dir (str): Directory to save the dummy files.\n",
        "        num_subjects (int): Number of dummy subjects to create.\n",
        "        num_frames (int): Number of frames per video (should be >120 for trimming logic).\n",
        "    \"\"\"\n",
        "    # 1. Setup Directory\n",
        "    if os.path.exists(output_dir):\n",
        "        shutil.rmtree(output_dir)\n",
        "    os.makedirs(output_dir)\n",
        "    print(f\"Created directory: {output_dir}\")\n",
        "\n",
        "    # 2. Generate Meta Data\n",
        "    # IDs starting from 1001\n",
        "    video_ids = [1001 + i for i in range(num_subjects)]\n",
        "    # Assign 0 (control) for the first half and 1 (autism) for the second half\n",
        "    subject_types = [0] * (num_subjects // 2) + [1] * (num_subjects // 2)\n",
        "\n",
        "    df_meta = pd.DataFrame({\n",
        "        'video_id': video_ids,\n",
        "        'subject_type': subject_types\n",
        "    })\n",
        "\n",
        "    meta_path = os.path.join(output_dir, 'meta_data.csv')\n",
        "    df_meta.to_csv(meta_path, index=False)\n",
        "    print(f\"Generated metadata: {meta_path}\")\n",
        "\n",
        "    # 3. Generate Skeletal CSVs\n",
        "    # Each file mimics the [video_id]_[action].csv format\n",
        "    action_code = \"BP\" # Example action code from your prompt\n",
        "\n",
        "    for vid_id in video_ids:\n",
        "        # Create columns: video_id, action, Frame\n",
        "        data = {\n",
        "            'video_id': [vid_id] * num_frames,\n",
        "            'action': [action_code] * num_frames,\n",
        "            'Frame': list(range(num_frames))\n",
        "        }\n",
        "\n",
        "        # Create Joint Columns 0 to 23\n",
        "        # Format: String representation of [x, y, z] e.g., \"[0.5, 0.2, 0.1]\"\n",
        "        for j in range(24): # 24 joints as per manuscript [cite: 95]\n",
        "            # Generate random 3D coordinates for this joint across all frames\n",
        "            # Using random floats between -1 and 1 to simulate normalized space\n",
        "            coords = np.random.rand(num_frames, 3).round(4).tolist()\n",
        "            # Convert list to string format \"[x, y, z]\" to match ast.literal_eval expectation\n",
        "            data[str(j)] = [str(c) for c in coords]\n",
        "\n",
        "        df_skeleton = pd.DataFrame(data)\n",
        "\n",
        "        # Save file\n",
        "        filename = f\"{vid_id}_{action_code}.csv\"\n",
        "        file_path = os.path.join(output_dir, filename)\n",
        "        df_skeleton.to_csv(file_path, index=False)\n",
        "        print(f\"Generated skeletal file: {file_path}\")\n",
        "\n",
        "    print(\"\\nDummy dataset generation complete.\")\n",
        "    print(f\"Example usage in pipeline: \\n  control_train_files = ['{output_dir}/1001_BP.csv', ...]\")\n",
        "\n",
        "# --- Execute Generation ---\n",
        "generate_dummy_dataset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vh7VFbHodnls",
        "outputId": "6f72ba8e-1eba-42d2-84b3-678cdc349957"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created directory: sample_data\n",
            "Generated metadata: sample_data/meta_data.csv\n",
            "Generated skeletal file: sample_data/1001_BP.csv\n",
            "Generated skeletal file: sample_data/1002_BP.csv\n",
            "Generated skeletal file: sample_data/1003_BP.csv\n",
            "Generated skeletal file: sample_data/1004_BP.csv\n",
            "\n",
            "Dummy dataset generation complete.\n",
            "Example usage in pipeline: \n",
            "  control_train_files = ['sample_data/1001_BP.csv', ...]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Core Processing Logic\n",
        "The following functions handle the parsing of string-formatted coordinates from CSVs and the generation of sliding windows."
      ],
      "metadata": {
        "id": "Z4ymK9MpbN4t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FuvuGJ6TyV1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from torch_geometric.data import Data\n",
        "import ast\n",
        "from itertools import permutations\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "def transform_to_graphs(csv_path, meta_data_path):\n",
        "    \"\"\"\n",
        "    Transforms time-series joint data from a CSV file into a collection of\n",
        "    PyTorch Geometric graphs.\n",
        "\n",
        "    Args:\n",
        "        csv_path (str): The file path to the input CSV file.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of torch_geometric.data.Data objects, where each object\n",
        "              represents a graph for a single frame.\n",
        "    \"\"\"\n",
        "    # --- 1. Validate paths and read data ---\n",
        "    if not os.path.exists(csv_path):\n",
        "        return f\"Error: The file '{csv_path}' was not found.\"\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path)\n",
        "        df_metadata = pd.read_csv(meta_data_path)\n",
        "    except Exception as e:\n",
        "        return f\"Error reading the CSV file: {e}\"\n",
        "\n",
        "    # --- 2. Extract video_id and get the corresponding label ---\n",
        "    if 'video_id' not in df.columns:\n",
        "        return \"Error: 'video_id' column not found in the input CSV.\"\n",
        "\n",
        "    # Assume the video_id is the same for all rows in the file\n",
        "    video_id = df['video_id'].iloc[0]\n",
        "\n",
        "    # Find the matching row in the metadata DataFrame\n",
        "    meta_row = df_metadata[df_metadata['video_id'] == video_id]\n",
        "\n",
        "    if meta_row.empty:\n",
        "        return f\"Error: video_id '{video_id}' not found in '{meta_data_path}'.\"\n",
        "\n",
        "    # Get the subject_type\n",
        "    subject_type = meta_row['subject_type'].iloc[0]\n",
        "    y_value = subject_type\n",
        "\n",
        "    # The label `y` is a graph-level target\n",
        "    y = torch.tensor([y_value], dtype=torch.long)\n",
        "\n",
        "    # --- 3. Process frames and create graphs ---\n",
        "    graph_collection = []\n",
        "    num_nodes = 24  # 24 joints\n",
        "\n",
        "    # Create the edge_index for a fully connected graph\n",
        "    # This can be created once and reused for all graphs since the number of nodes is constant.\n",
        "    perm = torch.tensor(list(permutations(range(num_nodes), 2)), dtype=torch.long)\n",
        "    edge_index = perm.t().contiguous()\n",
        "\n",
        "    # Iterate over each row in the DataFrame (each row is a frame)\n",
        "    for index, row in df.iterrows():\n",
        "        # Extract joint coordinates (columns '0' to '23')\n",
        "        # The joint coordinates are stored as strings, so we need to parse them.\n",
        "        node_features = []\n",
        "        for i in range(num_nodes):\n",
        "            joint_str = row[str(i)]\n",
        "            try:\n",
        "                # Safely evaluate the string to a list of coordinates\n",
        "                joint_coords = ast.literal_eval(joint_str)\n",
        "                node_features.append(joint_coords)\n",
        "            except (ValueError, SyntaxError):\n",
        "                # Handle cases where the string is not a valid list\n",
        "                print(f\"Warning: Could not parse joint data for row {index}, joint {i}. Skipping row. {video_id}\")\n",
        "                continue\n",
        "\n",
        "        # Ensure we have the correct number of features before creating a tensor\n",
        "        if len(node_features) != num_nodes:\n",
        "            continue\n",
        "\n",
        "        # Convert the list of node features to a PyTorch tensor\n",
        "        x = torch.tensor(node_features, dtype=torch.float)\n",
        "\n",
        "        # Get the frame number to store as a graph-level attribute\n",
        "        frame = row['Frame'] if 'Frame' in row else index\n",
        "\n",
        "        # Create a PyTorch Geometric Data object\n",
        "        data = Data(x=x, edge_index=edge_index, y=y, frame=frame)\n",
        "\n",
        "        # Add the graph to our collection\n",
        "        graph_collection.append(data)\n",
        "\n",
        "    return graph_collection\n",
        "\n",
        "def create_sliding_windows(graph_sequence, window_size, step):\n",
        "    \"\"\"\n",
        "    Takes a sequence of graphs and creates overlapping windows.\n",
        "\n",
        "    Args:\n",
        "        graph_sequence (list): A list of torch_geometric.data.Data objects.\n",
        "        window_size (int): The number of graphs in each window (timesteps).\n",
        "        step (int): The stride or step size between windows.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of windows, where each window is a list of graphs.\n",
        "    \"\"\"\n",
        "    windows = []\n",
        "    for i in range(0, len(graph_sequence) - window_size + 1, step):\n",
        "        window = graph_sequence[i: i + window_size]\n",
        "        windows.append(window)\n",
        "    return windows\n",
        "\n",
        "def trim_by_ending_frames(graph_sequence, num_frames_to_keep):\n",
        "    \"\"\"\n",
        "    Trims a sequence of graphs to keep only the final N frames.\n",
        "\n",
        "    This is based on the assumption that the video is cut shortly after\n",
        "    the main action is completed, making the end frames the most relevant.\n",
        "\n",
        "    Args:\n",
        "        graph_sequence (list): A list of torch_geometric.data.Data objects.\n",
        "        num_frames_to_keep (int): The number of frames to keep from the end.\n",
        "\n",
        "    Returns:\n",
        "        list: The trimmed list of graphs containing up to the last N frames.\n",
        "    \"\"\"\n",
        "    # Get the total number of frames in the sequence\n",
        "    total_frames = len(graph_sequence)\n",
        "\n",
        "    # If the video is already shorter than or equal to the desired number of frames,\n",
        "    # we don't need to do anything. Just return the whole sequence.\n",
        "    if total_frames <= num_frames_to_keep:\n",
        "        return graph_sequence\n",
        "\n",
        "    # Otherwise, calculate the starting index for the slice.\n",
        "    # For example, if we have 200 frames and want to keep 150,\n",
        "    # the start index will be 200 - 150 = 50. The slice will be from 50 to the end.\n",
        "    start_index = total_frames - num_frames_to_keep\n",
        "\n",
        "    # Return the slice from the calculated start index to the end of the list.\n",
        "    return graph_sequence[start_index:]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Custom STGCN Dataset Class\n",
        "We define a custom `torch.utils.data.Dataset` that wraps the preprocessing logic. This class:\n",
        "1. Iterates through a list of file paths.\n",
        "2. Applies the graph transformation and trimming.\n",
        "3. Stacks the graph snapshots into 3D tensors of shape `[Nodes, Features, Time]`."
      ],
      "metadata": {
        "id": "i_YP4n1YbaqE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9S4k2IZRUABi"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.data import Data\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "# --- --- --- --- NEW DATASET CLASS --- --- --- ---\n",
        "\n",
        "class STGCN_Dataset(Dataset):\n",
        "    \"\"\"\n",
        "    A PyTorch Dataset for loading spatio-temporal graph windows.\n",
        "\n",
        "    Args:\n",
        "        data_dir (str): Path to the directory containing the CSV data files.\n",
        "        meta_data_path (str): Path to the metadata CSV file.\n",
        "        window_size (int): The number of time steps (frames) in each window.\n",
        "        step (int): The stride between consecutive windows.\n",
        "    \"\"\"\n",
        "    def __init__(self, file_paths, meta_data_path, window_size, step):\n",
        "        super(STGCN_Dataset, self).__init__()\n",
        "        self.window_size = window_size\n",
        "        self.step = step\n",
        "        self.samples = []\n",
        "        self._edge_index = None\n",
        "\n",
        "        print(\"Processing data files...\")\n",
        "\n",
        "        # --- MODIFICATION ---\n",
        "        # Instead of iterating over a directory, we iterate over the provided list of file paths.\n",
        "        for filepath in file_paths:\n",
        "            # We no longer need os.path.join since `filepath` is the full path\n",
        "            video_id = os.path.splitext(os.path.basename(filepath))[0]\n",
        "\n",
        "            # 1. Transform the entire CSV into a sequence of graphs\n",
        "            graph_sequence = transform_to_graphs(filepath, meta_data_path)\n",
        "\n",
        "            graph_sequence = trim_by_ending_frames(graph_sequence, 120)\n",
        "\n",
        "            # if isinstance(graph_sequence, list) and len(graph_sequence) >= self.window_size:\n",
        "            if len(graph_sequence) >= self.window_size:\n",
        "                # 2. Create sliding windows from the graph sequence\n",
        "                windows = create_sliding_windows(graph_sequence, self.window_size, self.step)\n",
        "                for window in windows:\n",
        "                    self.samples.append({'window': window, 'video_id': video_id})\n",
        "\n",
        "                # Store the edge_index (it's the same for all)\n",
        "                if self._edge_index is None and graph_sequence:\n",
        "                    self._edge_index = graph_sequence[0].edge_index\n",
        "            else:\n",
        "                # This block will catch error strings and print a helpful warning\n",
        "                print(f\"  - WARNING: Skipping file '{os.path.basename(filepath)}'. Reason: {graph_sequence}\")\n",
        "\n",
        "        print(f\"Finished processing. Found {len(self.samples)} total training windows for this dataset.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Returns the total number of samples (windows).\"\"\"\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Generates one sample of data.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing:\n",
        "                - X (torch.Tensor): Node features for the window, with shape\n",
        "                                    [num_nodes, num_features, window_size].\n",
        "                - y (torch.Tensor): The label for the window.\n",
        "        \"\"\"\n",
        "        # Get the window (a list of Data objects)\n",
        "        sample = self.samples[idx]\n",
        "        window = sample['window']\n",
        "        video_id = sample['video_id']\n",
        "\n",
        "        # Stack node features along a new dimension to create the [N, F, T] tensor\n",
        "        # N=num_nodes, F=num_features, T=timesteps (window_size)\n",
        "        X = torch.stack([graph.x for graph in window], dim=2)\n",
        "\n",
        "        X_permuted = X.permute(2, 0, 1)\n",
        "\n",
        "        # The label is the same for all graphs in the window\n",
        "        y = window[0].y\n",
        "\n",
        "        return X_permuted, y, video_id\n",
        "\n",
        "    def get_edge_index(self):\n",
        "        \"\"\"Helper function to get the constant edge_index.\"\"\"\n",
        "        return self._edge_index"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Data Loading & Configuration\n",
        "\n",
        "**Note regarding data privacy:** The raw skeletal data files are not included in this repository. The paths below are placeholders. To run this locally, point `file_paths` to your directory containing MediaPipe `.csv` exports.\n",
        "\n",
        "**Hyperparameters:**\n",
        "* **Window Size:** 60 frames (2 seconds)\n",
        "* **Step Size:** 15 frames (0.5 seconds)"
      ],
      "metadata": {
        "id": "humPXviubhBp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration ---\n",
        "META_PATH = '/content/sample_data/meta_data.csv'  # Ensure a dummy metadata file exists in your repo\n",
        "WINDOW_SIZE = 60\n",
        "STEP = 15\n",
        "\n",
        "# --- Placeholder File Lists ---\n",
        "# In a real scenario, you would populate these lists using glob or os.listdir\n",
        "# e.g., control_train_files = glob.glob(\"data/control_train/*.csv\")\n",
        "\n",
        "control_train_files = ['/content/sample_data/1001_BP.csv'] # [Add path to sample_control.csv provided in repo]\n",
        "control_val_files = ['/content/sample_data/1002_BP.csv']\n",
        "autism_test_files = ['/content/sample_data/1003_BP.csv', '/content/sample_data/1004_BP.csv']   # [Add path to sample_asd.csv provided in repo]\n",
        "\n",
        "print(f\"Configuration Set: Window={WINDOW_SIZE}, Step={STEP}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ie4qf-EbsBS",
        "outputId": "32b8d602-45ca-43b1-a178-f065697ecf06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration Set: Window=60, Step=15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Instantiating Datasets\n",
        "Here we process the raw files into ready-to-train datasets."
      ],
      "metadata": {
        "id": "_yEuSJ-kb0bS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Only run this if files are present\n",
        "if len(control_train_files) > 0:\n",
        "    print(\"--- Processing Control Training Set ---\")\n",
        "    control_dataset_train = STGCN_Dataset(\n",
        "        file_paths=control_train_files,\n",
        "        meta_data_path=META_PATH,\n",
        "        window_size=WINDOW_SIZE,\n",
        "        step=STEP\n",
        "    )\n",
        "\n",
        "    print(\"\\n--- Processing Control Validation Set ---\")\n",
        "    control_dataset_val = STGCN_Dataset(\n",
        "        file_paths=control_val_files,\n",
        "        meta_data_path=META_PATH,\n",
        "        window_size=WINDOW_SIZE,\n",
        "        step=STEP\n",
        "    )\n",
        "\n",
        "    print(\"\\n--- Processing ASD Test Set ---\")\n",
        "    autism_dataset_test = STGCN_Dataset(\n",
        "        file_paths=autism_test_files,\n",
        "        meta_data_path=META_PATH,\n",
        "        window_size=WINDOW_SIZE,\n",
        "        step=STEP\n",
        "    )\n",
        "else:\n",
        "    print(\"No data files found. Please populate 'control_train_files' list.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjp8HyZ4b1L-",
        "outputId": "d41784e9-cfc4-47a3-9e5c-49e49d9493ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Processing Control Training Set ---\n",
            "Processing data files...\n",
            "Finished processing. Found 5 total training windows for this dataset.\n",
            "\n",
            "--- Processing Control Validation Set ---\n",
            "Processing data files...\n",
            "Finished processing. Found 5 total training windows for this dataset.\n",
            "\n",
            "--- Processing ASD Test Set ---\n",
            "Processing data files...\n",
            "Finished processing. Found 10 total training windows for this dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Shape Verification\n",
        "It is critical to verify that the output tensors match the expected input shape for the STGCN-AE: `[Time, Nodes, Features]`."
      ],
      "metadata": {
        "id": "lldMFijpcDK2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwkt-xZ0k8FD",
        "outputId": "1e7b251c-0a67-4a7c-c6f6-84ce11e32b08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video ID: 1001_BP\n",
            "Window Shape: torch.Size([60, 24, 3])\n"
          ]
        }
      ],
      "source": [
        "if 'control_dataset_train' in locals() and len(control_dataset_train) > 0:\n",
        "    sample_x, sample_y, video_id = control_dataset_train[0]\n",
        "\n",
        "    print(f\"Video ID: {video_id}\")\n",
        "    print(f\"Window Shape: {sample_x.shape}\")\n",
        "    # Expected: [60, 24, 3] (Time, Nodes, Features)\n",
        "    # Note: The Dataloader will add the Batch dimension later."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}